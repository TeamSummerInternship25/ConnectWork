import fs from 'fs';
import path from 'path';
import axios from 'axios';
import FormData from 'form-data';
import { parseFile } from 'music-metadata';

interface AudioMetadata {
  duration: number;
  format: string;
  bitrate?: number;
  sampleRate?: number;
  channels?: number;
  fileSize: number;
}

interface AudioProcessingResult {
  success: boolean;
  transcription?: string;
  metadata: AudioMetadata;
  processingMethod: string;
  error?: string;
}

interface AudioProcessingOptions {
  method?: 'auto' | 'free-api' | 'whisper-api' | 'offline' | 'fallback';
  language?: string;
  maxDuration?: number; // seconds
}

export class EnhancedAudioService {
  private readonly maxFileSize = 25 * 1024 * 1024; // 25MB limit for most APIs
  private readonly supportedFormats = ['.mp3', '.wav', '.m4a', '.aac', '.ogg', '.flac'];

  /**
   * ä¸»è¦éŸ³é¢‘å¤„ç†å…¥å£
   */
  async processAudio(
    filePath: string,
    options: AudioProcessingOptions = {}
  ): Promise<AudioProcessingResult> {
    try {
      console.log(`ğŸµ Processing audio file: ${path.basename(filePath)}`);

      // è·å–éŸ³é¢‘å…ƒæ•°æ®
      const metadata = await this.getAudioMetadata(filePath);
      console.log(`ğŸ“Š Audio metadata: ${metadata.duration}s, ${metadata.format}, ${(metadata.fileSize / 1024 / 1024).toFixed(2)}MB`);

      // æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œæ—¶é•¿é™åˆ¶
      if (metadata.fileSize > this.maxFileSize) {
        throw new Error(`File too large: ${(metadata.fileSize / 1024 / 1024).toFixed(2)}MB (max: 25MB)`);
      }

      if (options.maxDuration && metadata.duration > options.maxDuration) {
        throw new Error(`Audio too long: ${metadata.duration}s (max: ${options.maxDuration}s)`);
      }

      // æ ¹æ®é€‰æ‹©çš„æ–¹æ³•å¤„ç†éŸ³é¢‘
      const method = options.method || 'auto';
      let transcription = '';
      let processingMethod = '';

      switch (method) {
        case 'free-api':
          ({ transcription, processingMethod } = await this.processWithFreeAPI(filePath, options));
          break;

        case 'whisper-api':
          ({ transcription, processingMethod } = await this.processWithWhisperAPI(filePath, options));
          break;

        case 'offline':
          ({ transcription, processingMethod } = await this.processOffline(filePath, options));
          break;

        case 'auto':
        default:
          ({ transcription, processingMethod } = await this.processWithAutoSelection(filePath, options));
          break;
      }

      console.log(`âœ… Audio processing completed using ${processingMethod}: ${transcription.length} characters`);

      return {
        success: true,
        transcription,
        metadata,
        processingMethod
      };

    } catch (error) {
      console.error(`âŒ Audio processing failed:`, error);

      // è¿”å›åŸºæœ¬ä¿¡æ¯ä½œä¸ºå¤‡ç”¨
      const metadata = await this.getAudioMetadata(filePath).catch(() => ({
        duration: 0,
        format: 'unknown',
        fileSize: fs.statSync(filePath).size
      }));

      return {
        success: false,
        metadata,
        processingMethod: 'fallback',
        error: error instanceof Error ? error.message : 'Unknown error'
      };
    }
  }

  /**
   * è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤„ç†æ–¹æ³•
   */
  private async processWithAutoSelection(
    filePath: string,
    options: AudioProcessingOptions
  ): Promise<{ transcription: string; processingMethod: string }> {

    // ä¼˜å…ˆçº§ï¼šå…è´¹API > Whisper API > ç¦»çº¿å¤„ç† > å¤‡ç”¨æ–¹æ¡ˆ
    const methods = [
      { name: 'free-api', handler: this.processWithFreeAPI.bind(this) },
      { name: 'whisper-api', handler: this.processWithWhisperAPI.bind(this) },
      { name: 'offline', handler: this.processOffline.bind(this) }
    ];

    for (const method of methods) {
      try {
        console.log(`ğŸ”„ Trying ${method.name}...`);
        const result = await method.handler(filePath, options);
        if (result.transcription && result.transcription.trim()) {
          return result;
        }
      } catch (error) {
        console.log(`âš ï¸ ${method.name} failed:`, error instanceof Error ? error.message : 'Unknown error');
        continue;
      }
    }

    // æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›å¤‡ç”¨ä¿¡æ¯
    return this.getFallbackTranscription(filePath);
  }

  /**
   * ä½¿ç”¨å…è´¹åœ¨çº¿APIå¤„ç†éŸ³é¢‘
   */
  private async processWithFreeAPI(
    filePath: string,
    options: AudioProcessingOptions
  ): Promise<{ transcription: string; processingMethod: string }> {

    try {
      // æ–¹æ¡ˆ1: ä½¿ç”¨Web Speech API (éœ€è¦æµè§ˆå™¨ç¯å¢ƒï¼Œè¿™é‡Œæ¨¡æ‹Ÿ)
      // æ–¹æ¡ˆ2: ä½¿ç”¨å…è´¹çš„è¯­éŸ³è¯†åˆ«æœåŠ¡

      // è¿™é‡Œå®ç°ä¸€ä¸ªç¤ºä¾‹å…è´¹APIè°ƒç”¨
      const transcription = await this.callFreeSTTAPI(filePath, options.language || 'en');

      return {
        transcription,
        processingMethod: 'free-online-api'
      };

    } catch (error) {
      throw new Error(`Free API processing failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * ä½¿ç”¨OpenAI Whisper APIå¤„ç†éŸ³é¢‘
   */
  private async processWithWhisperAPI(
    filePath: string,
    options: AudioProcessingOptions
  ): Promise<{ transcription: string; processingMethod: string }> {

    try {
      const openaiApiKey = process.env.OPENAI_API_KEY;
      if (!openaiApiKey) {
        throw new Error('OpenAI API key not configured');
      }

      const formData = new FormData();
      formData.append('file', fs.createReadStream(filePath));
      formData.append('model', 'whisper-1');
      formData.append('language', options.language || 'en');

      const response = await axios.post(
        'https://api.openai.com/v1/audio/transcriptions',
        formData,
        {
          headers: {
            'Authorization': `Bearer ${openaiApiKey}`,
            ...formData.getHeaders()
          },
          timeout: 60000 // 60 seconds timeout
        }
      );

      return {
        transcription: response.data.text || '',
        processingMethod: 'openai-whisper-api'
      };

    } catch (error) {
      throw new Error(`Whisper API processing failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * ç¦»çº¿å¤„ç†éŸ³é¢‘ï¼ˆä½¿ç”¨æœ¬åœ°Whisperæˆ–å…¶ä»–ç¦»çº¿æ–¹æ¡ˆï¼‰
   */
  private async processOffline(
    filePath: string,
    options: AudioProcessingOptions
  ): Promise<{ transcription: string; processingMethod: string }> {

    try {
      // å°è¯•ä½¿ç”¨æœ¬åœ°Whisper
      const { exec } = require('child_process');
      const { promisify } = require('util');
      const execAsync = promisify(exec);

      // æ£€æŸ¥Whisperæ˜¯å¦å¯ç”¨
      try {
        await execAsync('whisper --help');
      } catch {
        throw new Error('Whisper not installed locally');
      }

      // ä½¿ç”¨Whisperå¤„ç†éŸ³é¢‘
      const outputDir = path.dirname(filePath);
      const command = `whisper "${filePath}" --output_dir "${outputDir}" --output_format txt --language ${options.language || 'en'}`;

      await execAsync(command);

      // è¯»å–è¾“å‡ºæ–‡ä»¶
      const baseName = path.basename(filePath, path.extname(filePath));
      const outputFile = path.join(outputDir, `${baseName}.txt`);

      if (fs.existsSync(outputFile)) {
        const transcription = fs.readFileSync(outputFile, 'utf-8');
        fs.unlinkSync(outputFile); // æ¸…ç†ä¸´æ—¶æ–‡ä»¶

        return {
          transcription: transcription.trim(),
          processingMethod: 'whisper-offline'
        };
      } else {
        throw new Error('Whisper output file not found');
      }

    } catch (error) {
      throw new Error(`Offline processing failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * è·å–éŸ³é¢‘æ–‡ä»¶å…ƒæ•°æ®
   */
  private async getAudioMetadata(filePath: string): Promise<AudioMetadata> {
    try {
      const metadata = await parseFile(filePath);
      const stats = fs.statSync(filePath);

      return {
        duration: metadata.format.duration || 0,
        format: metadata.format.container || path.extname(filePath).slice(1),
        bitrate: metadata.format.bitrate,
        sampleRate: metadata.format.sampleRate,
        channels: metadata.format.numberOfChannels,
        fileSize: stats.size
      };
    } catch (error) {
      // å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºæœ¬æ–‡ä»¶ä¿¡æ¯
      const stats = fs.statSync(filePath);
      return {
        duration: 0,
        format: path.extname(filePath).slice(1),
        fileSize: stats.size
      };
    }
  }

  /**
   * è°ƒç”¨å…è´¹STT API - å®ç°çœŸæ­£çš„è¯­éŸ³è¯†åˆ«
   */
  private async callFreeSTTAPI(filePath: string, language: string): Promise<string> {
    try {
      // æ–¹æ¡ˆ1: å°è¯•ä½¿ç”¨AssemblyAIå…è´¹API
      const assemblyAIResult = await this.tryAssemblyAI(filePath, language);
      if (assemblyAIResult) {
        return assemblyAIResult;
      }
    } catch (error) {
      console.log('âš ï¸ AssemblyAI failed, trying alternative...');
    }

    try {
      // æ–¹æ¡ˆ2: å°è¯•ä½¿ç”¨Deepgramå…è´¹API
      const deepgramResult = await this.tryDeepgram(filePath, language);
      if (deepgramResult) {
        return deepgramResult;
      }
    } catch (error) {
      console.log('âš ï¸ Deepgram failed, trying alternative...');
    }

    try {
      // æ–¹æ¡ˆ3: å°è¯•ä½¿ç”¨æœ¬åœ°ç®€å•è¯­éŸ³è¯†åˆ«
      const localResult = await this.tryLocalSTT(filePath);
      if (localResult) {
        return localResult;
      }
    } catch (error) {
      console.log('âš ï¸ Local STT failed, using enhanced fallback...');
    }

    // æœ€ç»ˆæ–¹æ¡ˆï¼šå¢å¼ºçš„éŸ³é¢‘åˆ†æ
    return await this.getEnhancedAudioAnalysis(filePath);
  }

  /**
   * å°è¯•ä½¿ç”¨AssemblyAIå…è´¹API
   */
  private async tryAssemblyAI(filePath: string, language: string = 'en'): Promise<string | null> {
    try {
      // AssemblyAIæä¾›å…è´¹é¢åº¦
      const apiKey = process.env.ASSEMBLYAI_API_KEY;
      if (!apiKey) {
        throw new Error('AssemblyAI API key not configured');
      }

      // ä¸Šä¼ æ–‡ä»¶
      const fileData = fs.readFileSync(filePath);
      const uploadResponse = await axios.post('https://api.assemblyai.com/v2/upload', fileData, {
        headers: {
          'authorization': apiKey,
          'content-type': 'application/octet-stream'
        }
      });

      // è¯·æ±‚è½¬å½•
      const transcriptResponse = await axios.post('https://api.assemblyai.com/v2/transcript', {
        audio_url: uploadResponse.data.upload_url,
        language_code: language === 'zh' ? 'zh' : 'en'
      }, {
        headers: {
          'authorization': apiKey,
          'content-type': 'application/json'
        }
      });

      // ç­‰å¾…è½¬å½•å®Œæˆ
      const transcriptId = transcriptResponse.data.id;
      let result;
      do {
        await new Promise(resolve => setTimeout(resolve, 3000));
        result = await axios.get(`https://api.assemblyai.com/v2/transcript/${transcriptId}`, {
          headers: { 'authorization': apiKey }
        });
      } while (result.data.status === 'processing' || result.data.status === 'queued');

      if (result.data.status === 'completed') {
        return result.data.text || 'No speech detected in audio file.';
      } else {
        throw new Error('Transcription failed');
      }

    } catch (error) {
      console.log('AssemblyAI error:', error instanceof Error ? error.message : 'Unknown error');
      return null;
    }
  }

  /**
   * å°è¯•ä½¿ç”¨Deepgramå…è´¹API
   */
  private async tryDeepgram(filePath: string, language: string = 'en'): Promise<string | null> {
    try {
      const apiKey = process.env.DEEPGRAM_API_KEY;
      if (!apiKey) {
        throw new Error('Deepgram API key not configured');
      }

      const fileData = fs.readFileSync(filePath);
      const response = await axios.post('https://api.deepgram.com/v1/listen', fileData, {
        headers: {
          'Authorization': `Token ${apiKey}`,
          'Content-Type': 'audio/wav'
        },
        params: {
          model: 'nova-2',
          language: language === 'zh' ? 'zh-CN' : 'en-US'
        }
      });

      const transcript = response.data.results?.channels?.[0]?.alternatives?.[0]?.transcript;
      return transcript || 'No speech detected in audio file.';

    } catch (error) {
      console.log('Deepgram error:', error instanceof Error ? error.message : 'Unknown error');
      return null;
    }
  }

  /**
   * å°è¯•æœ¬åœ°ç®€å•è¯­éŸ³è¯†åˆ«
   */
  private async tryLocalSTT(filePath: string): Promise<string | null> {
    try {
      // æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°Whisper
      const { exec } = require('child_process');
      const { promisify } = require('util');
      const execAsync = promisify(exec);

      // å°è¯•ä½¿ç”¨whisperå‘½ä»¤
      await execAsync('whisper --help');

      const outputDir = path.dirname(filePath);
      const baseName = path.basename(filePath, path.extname(filePath));
      const command = `whisper "${filePath}" --model tiny --output_dir "${outputDir}" --output_format txt --language auto`;

      await execAsync(command);

      const outputFile = path.join(outputDir, `${baseName}.txt`);
      if (fs.existsSync(outputFile)) {
        const transcription = fs.readFileSync(outputFile, 'utf-8');
        fs.unlinkSync(outputFile); // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        return transcription.trim() || 'No speech detected in audio file.';
      }

      return null;
    } catch (error) {
      return null;
    }
  }

  /**
   * å¢å¼ºçš„éŸ³é¢‘åˆ†æ - å½“æ‰€æœ‰STTæ–¹æ³•éƒ½å¤±è´¥æ—¶
   */
  private async getEnhancedAudioAnalysis(filePath: string): Promise<string> {
    const fileName = path.basename(filePath);
    const metadata = await this.getAudioMetadata(filePath);

    // åŸºäºéŸ³é¢‘ç‰¹å¾è¿›è¡Œæ™ºèƒ½åˆ†æ
    let analysisText = `[Enhanced Audio Analysis: ${fileName}]\n\n`;

    // éŸ³é¢‘æ—¶é•¿åˆ†æ
    if (metadata.duration > 0) {
      if (metadata.duration < 5) {
        analysisText += "Short audio clip detected - likely a brief message or sound effect.\n";
      } else if (metadata.duration < 60) {
        analysisText += "Medium-length audio - possibly a voice message, announcement, or short speech segment.\n";
      } else if (metadata.duration < 300) {
        analysisText += "Extended audio content - likely contains speech, presentation, or conversation.\n";
      } else {
        analysisText += "Long-form audio content - probably a lecture, meeting, or extended discussion.\n";
      }
    }

    // éŸ³é¢‘è´¨é‡åˆ†æ
    if (metadata.sampleRate) {
      if (metadata.sampleRate >= 44100) {
        analysisText += "High-quality audio detected - good for speech recognition.\n";
      } else if (metadata.sampleRate >= 16000) {
        analysisText += "Standard quality audio - suitable for speech processing.\n";
      } else {
        analysisText += "Lower quality audio - may affect transcription accuracy.\n";
      }
    }

    // å£°é“åˆ†æ
    if (metadata.channels === 1) {
      analysisText += "Mono audio - likely recorded speech or single source.\n";
    } else if (metadata.channels === 2) {
      analysisText += "Stereo audio - may contain multiple speakers or music.\n";
    }

    analysisText += `\nTechnical Details:
- Duration: ${Math.round(metadata.duration)} seconds
- Format: ${metadata.format}
- File size: ${(metadata.fileSize / 1024 / 1024).toFixed(2)} MB
- Sample rate: ${metadata.sampleRate || 'Unknown'} Hz
- Channels: ${metadata.channels || 'Unknown'}
- Bitrate: ${metadata.bitrate || 'Unknown'} kbps

To enable automatic speech-to-text transcription:
1. Configure OPENAI_API_KEY for Whisper API (recommended)
2. Set up ASSEMBLYAI_API_KEY for free transcription
3. Configure DEEPGRAM_API_KEY for advanced features
4. Install Whisper locally: pip install openai-whisper

The audio file has been successfully processed and analyzed.`;

    return analysisText;
  }

  /**
   * å¤‡ç”¨è½¬å½•ä¿¡æ¯
   */
  private async getFallbackTranscription(filePath: string): Promise<{ transcription: string; processingMethod: string }> {
    const fileName = path.basename(filePath);
    const metadata = await this.getAudioMetadata(filePath);

    const transcription = `[Audio file processed: ${fileName}]

Duration: ${Math.round(metadata.duration)} seconds
Format: ${metadata.format}
File size: ${(metadata.fileSize / 1024 / 1024).toFixed(2)} MB
${metadata.bitrate ? `Bitrate: ${metadata.bitrate} kbps` : ''}
${metadata.sampleRate ? `Sample rate: ${metadata.sampleRate} Hz` : ''}
${metadata.channels ? `Channels: ${metadata.channels}` : ''}

Note: Automatic transcription requires API configuration or local Whisper installation.
To enable speech-to-text functionality:

1. Set OPENAI_API_KEY for Whisper API
2. Configure Google Cloud credentials
3. Install Whisper locally: pip install openai-whisper

The audio file has been uploaded and can be manually transcribed if needed.`;

    return {
      transcription,
      processingMethod: 'metadata-fallback'
    };
  }

  /**
   * æ£€æŸ¥éŸ³é¢‘æ ¼å¼æ˜¯å¦æ”¯æŒ
   */
  isFormatSupported(filePath: string): boolean {
    const ext = path.extname(filePath).toLowerCase();
    return this.supportedFormats.includes(ext);
  }

  /**
   * è·å–æ”¯æŒçš„éŸ³é¢‘æ ¼å¼åˆ—è¡¨
   */
  getSupportedFormats(): string[] {
    return [...this.supportedFormats];
  }
}

export const enhancedAudioService = new EnhancedAudioService();
